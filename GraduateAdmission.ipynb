{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnLly5JQbC45RZFlIKFEFX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valar-Melkor/Deep-Learning-Projects/blob/main/GraduateAdmission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mS-P264oZw23"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow\timport keras\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import layers\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the features and labels from the data and removing the irrlevent features then dividing the data into train and test sets in order to perform regression analysis."
      ],
      "metadata": {
        "id": "akgcbtenc9zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/admissions_data.csv')\n",
        "\n",
        "features = df.iloc[:, 0:-1]\n",
        "labels = df.iloc[:, -1]\n",
        "\n",
        "features = features.drop(['Serial No.'], axis = 1)\n",
        "\n",
        "\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.2)"
      ],
      "metadata": {
        "id": "B_i3JoB6aISv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling the data so that that all the columns have equal weight"
      ],
      "metadata": {
        "id": "S-nue_Bpec35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "features_train = scaler.fit_transform(features_train)\n",
        "features_test = scaler.transform(features_test)\n"
      ],
      "metadata": {
        "id": "9GJZay9kc757"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the deep learning sequential model with 2 hidden layers."
      ],
      "metadata": {
        "id": "b29cizmBekql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "input_layer = layers.InputLayer(input_shape = (features_train.shape[1], ))\n",
        "model.add(input_layer)\n",
        "hidden_layer_1 = layers.Dense(32, activation = 'relu')\n",
        "model.add(hidden_layer_1)\n",
        "hidden_layer_2 = layers.Dense(16, activation = 'relu')\n",
        "model.add(hidden_layer_2)\n",
        "output_layer = layers.Dense(1)\n",
        "model.add(output_layer)\n",
        "\n",
        "opt = Adam(learning_rate = 0.001)\n",
        "\n",
        "model.compile(loss = 'mse', metrics = ['mae'], optimizer = opt)"
      ],
      "metadata": {
        "id": "1YwwzEo8eqhz"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and evaluating the deep learning model"
      ],
      "metadata": {
        "id": "G6Jc8w_9gHk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(features_train, labels_train, epochs = 90, verbose = 0, batch_size = 25)\n",
        "mse, mae = model.evaluate(features_test, labels_test, verbose = 0)\n",
        "print(mse,mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOXPnipVgPtd",
        "outputId": "68fad81b-7dc5-4a8b-8a6b-03c3c44ee715"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.004359828308224678 0.049575261771678925\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "0.7968062797973166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(features_test)\n",
        "print(r2_score(labels_test,predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO-q9YrJkl5C",
        "outputId": "1225bbf8-4bb0-422a-9826-9f40b3abb2eb"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "0.7968062797973166\n"
          ]
        }
      ]
    }
  ]
}